{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SageMaker Studio to ingest data from Delta Table\n",
    "\n",
    "In this SageMaker notebook, we will show how to access data created as a Delta Table (https://docs.delta.io/latest/index.html) and load it directly using the open source `delta-spark` library (https://github.com/delta-io/delta). We use the Dataframe API from the PySpark library to ingest and transform the dataset attributes. Finally, we show how to use the `DeltaTable` class to manipulate the underlying table structure.\n",
    "\n",
    "Prerequisites: First, we have to install some libraries into our Jupyter environment. These include:\n",
    "\n",
    "* install OpenJDK for access to Java and associated libraries\n",
    "* install Pyspark (Spark for Python) library\n",
    "* install Delta Spark open source library\n",
    "\n",
    "We will use either conda or pip to install these various libraries, which are publicly available in either conda-forge or maven repositories on the internet.\n",
    "\n",
    "Note 1: This notebook is designed to run within SageMaker Studio. \n",
    "Please make sure you select the `Python 3(Data Science)` kernel above. \n",
    "\n",
    "Note 2: Pyspark commands will run faster if you select an instance type with at least 16 GB of RAM (like `ml.g4dn.xlarge`).\n",
    "\n",
    "This notebook is covered under the MIT-0 License:\n",
    "* Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "* SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install openjdk -q -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pyspark==3.2.0 in /opt/conda/lib/python3.7/site-packages (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /opt/conda/lib/python3.7/site-packages (from pyspark==3.2.0) (0.10.9.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: delta-spark==1.1.0 in /opt/conda/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: pyspark<3.3.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from delta-spark==1.1.0) (3.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from delta-spark==1.1.0) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.0.0->delta-spark==1.1.0) (2.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /opt/conda/lib/python3.7/site-packages (from pyspark<3.3.0,>=3.2.0->delta-spark==1.1.0) (0.10.9.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install delta-spark==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: sagemaker>2.72 in /opt/conda/lib/python3.7/site-packages (2.75.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (0.1.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (1.20.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (1.0.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (0.2.8)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (19.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (20.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (3.19.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (0.2.0)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (1.20.23)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>2.72) (1.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>2.72) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>2.72) (1.23.23)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker>2.72) (0.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker>2.72) (2.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>2.72) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>2.72) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>2.72) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>2.72) (2019.3)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>2.72) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>2.72) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>2.72) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>2.72) (0.70.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.20.21->sagemaker>2.72) (1.26.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"sagemaker>2.72\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add hostname to instance /etc/hosts file\n",
    "\n",
    "Additionally, there is another command below which adds the resolved hostname to the /etc/hosts file on the notebook instance. Note: This code is a temporary work-around required until SageMaker releases a fix for this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hostname into /etc/hosts \n",
    "!grep `hostname` /etc/hosts >/dev/null || echo 127.0.0.1 `hostname` >> /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.75.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark Session to use additional library packages to satisfy dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages: io.delta:delta-core_2.12:1.1.0,org.apache.hadoop:hadoop-aws:3.2.2\n"
     ]
    }
   ],
   "source": [
    "# Build list of packages entries using Maven coordinates (groupId:artifactId:version)\n",
    "pkg_list = []\n",
    "pkg_list.append(\"io.delta:delta-core_2.12:1.1.0\")\n",
    "pkg_list.append(\"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "\n",
    "packages=(\",\".join(pkg_list))\n",
    "print('packages: '+packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Spark via builder\n",
    "# Note: we use the `ContainerCredentialsProvider` to give us access to underlying IAM role permissions\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"PySparkApp\") \n",
    "    .config(\"spark.jars.packages\", packages) \n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "    .config(\"fs.s3a.aws.credentials.provider\",'com.amazonaws.auth.ContainerCredentialsProvider') \n",
    "    .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print('Spark version: '+str(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lending Club Loans Data\n",
    "\n",
    "In this notebook, we will use a publicly available dataset from Lending Club that represents customer loan data. We have previously downloaded the \"accepted\" data file (accepted_2007_to_2018Q4.csv.gz\"), and have selected a small subset of attributes. This dataset is available under the Creative Commons (CCO) License (https://creativecommons.org/publicdomain/zero/1.0/). \n",
    "\n",
    "Lending Club Data: https://www.kaggle.com/wordsforthewise/lending-club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default bucket: sagemaker-us-east-1-572539092864\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# S3 bucket for saving processing job outputs\n",
    "sm_session = sagemaker.Session()\n",
    "bucket = sm_session.default_bucket()\n",
    "region = sm_session.boto_region_name\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "print('Default bucket: '+bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw CSV file to S3 for efficient access by PySpark\n",
    "\n",
    "Note: PySpark uses the 's3a' protocol to enable additional hadoop libraries. Therefore, we modify each native S3 URI to use 's3a' in the cells throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-572539092864/delta_demo/raw_csv/part-00000-tid-3730514337082854868-615b9e6a-1218-4f20-b930-a1fcfa6603a2-29-1-c000.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "local_basename = 'part-00000-tid-3730514337082854868-615b9e6a-1218-4f20-b930-a1fcfa6603a2-29-1-c000.csv'\n",
    "local_file = '../data/raw_csv/' + local_basename\n",
    "upload_s3_uri = f's3://{bucket}/delta_demo/raw_csv'\n",
    "\n",
    "S3Uploader.upload(local_file, upload_s3_uri, sagemaker_session=sm_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-us-east-1-572539092864/delta_demo/raw_csv/part-00000-tid-3730514337082854868-615b9e6a-1218-4f20-b930-a1fcfa6603a2-29-1-c000.csv\n"
     ]
    }
   ],
   "source": [
    "# Load raw data from S3 location\n",
    "\n",
    "s3_raw_csv = f's3://{bucket}/delta_demo/raw_csv/part-00000-tid-3730514337082854868-615b9e6a-1218-4f20-b930-a1fcfa6603a2-29-1-c000.csv'\n",
    "s3a_raw_csv = s3_raw_csv.replace('s3:','s3a:')\n",
    "\n",
    "print(s3a_raw_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.66 ms, sys: 204 Âµs, total: 3.86 ms\n",
      "Wall time: 5.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loans_df = spark.read.csv(s3a_raw_csv, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 49999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('loan_amnt', 'string'),\n",
       " ('funded_amnt', 'string'),\n",
       " ('term', 'string'),\n",
       " ('int_rate', 'string'),\n",
       " ('addr_state', 'string'),\n",
       " ('loan_status', 'string')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Rows: '+str(loans_df.count()))\n",
    "loans_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+---------+--------+----------+-----------+\n",
      "|      id|loan_amnt|funded_amnt|     term|int_rate|addr_state|loan_status|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+\n",
      "|68407277|   3600.0|     3600.0|36 months|   13.99|        PA| Fully Paid|\n",
      "|68355089|  24700.0|    24700.0|36 months|   11.99|        SD| Fully Paid|\n",
      "|68341763|  20000.0|    20000.0|60 months|   10.78|        IL| Fully Paid|\n",
      "|66310712|  35000.0|    35000.0|60 months|   14.85|        NJ|    Current|\n",
      "|68476807|  10400.0|    10400.0|60 months|   22.45|        PA| Fully Paid|\n",
      "|68426831|  11950.0|    11950.0|36 months|   13.44|        GA| Fully Paid|\n",
      "|68476668|  20000.0|    20000.0|36 months|    9.17|        MN| Fully Paid|\n",
      "|67275481|  20000.0|    20000.0|36 months|    8.49|        SC| Fully Paid|\n",
      "|68466926|  10000.0|    10000.0|36 months|    6.49|        PA| Fully Paid|\n",
      "|68616873|   8000.0|     8000.0|36 months|   11.48|        RI| Fully Paid|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loans_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark DataFrame, write out specifying the Delta Lake format\n",
    "\n",
    "Note: No upfront schema definition needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-us-east-1-572539092864/delta_demo/delta_format/\n"
     ]
    }
   ],
   "source": [
    "# Write dataframe to Delta Table location using 's3a' protocol\n",
    "\n",
    "s3_delta_table_uri=f's3://{bucket}/delta_demo/delta_format/'\n",
    "s3a_delta_table_uri=s3_delta_table_uri.replace('s3:','s3a:')\n",
    "\n",
    "print(s3a_delta_table_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df.write.format(\"delta\").mode(\"overwrite\").save(s3a_delta_table_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use Spark Sql commands to query table data\n",
    "\n",
    "Spark SQL can be run inline using SparkSession object. By passing a valid SQL syntax string, we can execute SQL commands which will return the result set as a Spark Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL command: SELECT * FROM delta.`s3a://sagemaker-us-east-1-572539092864/delta_demo/delta_format/` ORDER BY loan_amnt\n"
     ]
    }
   ],
   "source": [
    "# Create SQL command inserting the S3 path location\n",
    "\n",
    "sql_cmd = f'SELECT * FROM delta.`{s3a_delta_table_uri}` ORDER BY loan_amnt'\n",
    "print(f'SQL command: {sql_cmd}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+\n",
      "|      id|loan_amnt|funded_amnt|     term|int_rate|addr_state|loan_status|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+\n",
      "|68356556|   1000.0|     1000.0|36 months|   11.48|        NM| Fully Paid|\n",
      "|68355144|   1000.0|     1000.0|36 months|   17.97|        NY| Fully Paid|\n",
      "|67347650|   1000.0|     1000.0|36 months|   11.99|        NY| Fully Paid|\n",
      "|68436190|   1000.0|     1000.0|36 months|   13.99|        TN|Charged Off|\n",
      "|68356337|   1000.0|     1000.0|36 months|   12.88|        VA| Fully Paid|\n",
      "|68576654|   1000.0|     1000.0|36 months|   13.44|        NY| Fully Paid|\n",
      "|68395234|   1000.0|     1000.0|36 months|   12.88|        NY| Fully Paid|\n",
      "|68536214|   1000.0|     1000.0|36 months|   11.99|        TX| Fully Paid|\n",
      "|68465543|   1000.0|     1000.0|36 months|    7.49|        PA| Fully Paid|\n",
      "|67859219|   1000.0|     1000.0|36 months|   11.48|        FL| Fully Paid|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute SQL command which returns dataframe\n",
    "\n",
    "sql_results = spark.sql(sql_cmd)\n",
    "print(type(sql_results))\n",
    "\n",
    "sql_results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging DeltaTable API\n",
    "\n",
    "The following notebook cells demonstrate various capabilities when using the DeltaTable class (https://docs.delta.io/latest/api/python/index.html).\n",
    "\n",
    "\n",
    "The DeltaTable API allows us to retrieve the table modification history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "# Use static method to determine table type\n",
    "print(DeltaTable.isDeltaTable(spark, s3a_delta_table_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, s3a_delta_table_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(version=7, timestamp=datetime.datetime(2022, 2, 16, 18, 1, 3), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'Overwrite', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=6, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numOutputRows': '49999', 'numOutputBytes': '526896', 'numFiles': '1'}, userMetadata=None, engineInfo='Apache-Spark/3.2.0 Delta-Lake/1.1.0'),\n",
       " Row(version=6, timestamp=datetime.datetime(2022, 2, 9, 16, 37, 39), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'Overwrite', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=5, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numOutputRows': '49999', 'numOutputBytes': '526896', 'numFiles': '1'}, userMetadata=None, engineInfo='Apache-Spark/3.2.0 Delta-Lake/1.1.0'),\n",
       " Row(version=5, timestamp=datetime.datetime(2022, 2, 8, 20, 49, 24), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'Overwrite', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=4, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numOutputRows': '49999', 'numOutputBytes': '526896', 'numFiles': '1'}, userMetadata=None, engineInfo='Apache-Spark/3.2.0 Delta-Lake/1.1.0')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df = deltaTable.history()\n",
    "history_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate Schema Evolution\n",
    "\n",
    "First, let's read data back in from the Delta Table. Since this data was written out as `delta` format, we need to specify `.format(\"delta\")` when reading the data and then we provide the S3 URI where the Delta Table is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2.15 ms, total: 2.15 ms\n",
      "Wall time: 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# optional: include `versionAsOf` option to read specific version\n",
    "delta_df = (spark.read.format(\"delta\").load(s3a_delta_table_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49999"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we will write out the table with the existing schema to a new S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-us-east-1-572539092864/delta_demo/delta_schema_update/\n"
     ]
    }
   ],
   "source": [
    "# We will specify a new S3 Uri to store the original data frame with the original schema\n",
    "\n",
    "s3_delta_update_uri=f's3://{bucket}/delta_demo/delta_schema_update/'\n",
    "s3a_delta_update_uri = s3_delta_update_uri.replace('s3:','s3a:')\n",
    "print(s3a_delta_update_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3.12 ms, total: 3.12 ms\n",
      "Wall time: 5.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "delta_df.write.format(\"delta\").mode(\"overwrite\").save(s3a_delta_update_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's add two new columns to the schema using the existing Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('loan_amnt', 'string'),\n",
       " ('funded_amnt', 'string'),\n",
       " ('term', 'string'),\n",
       " ('int_rate', 'string'),\n",
       " ('addr_state', 'string'),\n",
       " ('loan_status', 'string'),\n",
       " ('funding_type', 'string'),\n",
       " ('excess_int_rate', 'double')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import (datediff, lit, col)\n",
    "\n",
    "funding_type_col = \"funding_type\"\n",
    "excess_int_rate_col = \"excess_int_rate\"\n",
    "\n",
    "delta_update_df = (delta_df.withColumn(funding_type_col, lit(\"NA\"))\n",
    "                           .withColumn(excess_int_rate_col, lit(0.0)))\n",
    "delta_update_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+---------+--------+----------+-----------+------------+---------------+\n",
      "|      id|loan_amnt|funded_amnt|     term|int_rate|addr_state|loan_status|funding_type|excess_int_rate|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+------------+---------------+\n",
      "|68407277|   3600.0|     3600.0|36 months|   13.99|        PA| Fully Paid|          NA|            0.0|\n",
      "|68355089|  24700.0|    24700.0|36 months|   11.99|        SD| Fully Paid|          NA|            0.0|\n",
      "|68341763|  20000.0|    20000.0|60 months|   10.78|        IL| Fully Paid|          NA|            0.0|\n",
      "|66310712|  35000.0|    35000.0|60 months|   14.85|        NJ|    Current|          NA|            0.0|\n",
      "|68476807|  10400.0|    10400.0|60 months|   22.45|        PA| Fully Paid|          NA|            0.0|\n",
      "|68426831|  11950.0|    11950.0|36 months|   13.44|        GA| Fully Paid|          NA|            0.0|\n",
      "|68476668|  20000.0|    20000.0|36 months|    9.17|        MN| Fully Paid|          NA|            0.0|\n",
      "|67275481|  20000.0|    20000.0|36 months|    8.49|        SC| Fully Paid|          NA|            0.0|\n",
      "|68466926|  10000.0|    10000.0|36 months|    6.49|        PA| Fully Paid|          NA|            0.0|\n",
      "|68616873|   8000.0|     8000.0|36 months|   11.48|        RI| Fully Paid|          NA|            0.0|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_update_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing table update to: s3a://sagemaker-us-east-1-572539092864/delta_demo/delta_schema_update/\n"
     ]
    }
   ],
   "source": [
    "print('Writing table update to: '+s3a_delta_update_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(delta_update_df.write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"mergeSchema\", \"true\") # option - evolve schema\n",
    " .save(s3a_delta_update_uri)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeltaTable Usage \n",
    "\n",
    "Let's check the modification history of our new table, which we will use to show schema evolution. The history will show each revision to the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's confirm that this S3 Uri path points to a DeltaTable\n",
    "\n",
    "DeltaTable.isDeltaTable(spark, s3a_delta_update_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the S3 Uri, we instantiate a DeltaTable instance\n",
    "\n",
    "deltaTableUpdate = DeltaTable.forPath(spark, s3a_delta_update_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      5|2022-02-16 18:02:45|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          4|  Serializable|        false|{numFiles -> 1, n...|        null|Apache-Spark/3.2....|\n",
      "|      4|2022-02-16 18:02:19|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          3|  Serializable|        false|{numFiles -> 1, n...|        null|Apache-Spark/3.2....|\n",
      "|      3|2022-02-16 17:57:58|  null|    null|   UPDATE|{predicate -> (ca...|null|    null|     null|          2|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.2....|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's retrieve history BEFORE schema modification\n",
    "\n",
    "history_update_df = deltaTableUpdate.history()\n",
    "history_update_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DeltaTable `update` method can be used to execute a predicate and then apply a transform whenever the condition evaluates to True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTableUpdate.update(condition = col(\"loan_amnt\") == col(\"funded_amnt\"),\n",
    " set = { funding_type_col: lit(\"FullyFunded\") } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_update_df = deltaTableUpdate.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+---------+--------+----------+-----------+------------+---------------+\n",
      "|      id|loan_amnt|funded_amnt|     term|int_rate|addr_state|loan_status|funding_type|excess_int_rate|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+------------+---------------+\n",
      "|68407277|   3600.0|     3600.0|36 months|   13.99|        PA| Fully Paid| FullyFunded|            0.0|\n",
      "|68355089|  24700.0|    24700.0|36 months|   11.99|        SD| Fully Paid| FullyFunded|            0.0|\n",
      "|68341763|  20000.0|    20000.0|60 months|   10.78|        IL| Fully Paid| FullyFunded|            0.0|\n",
      "|66310712|  35000.0|    35000.0|60 months|   14.85|        NJ|    Current| FullyFunded|            0.0|\n",
      "|68476807|  10400.0|    10400.0|60 months|   22.45|        PA| Fully Paid| FullyFunded|            0.0|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_update_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DeltaTable `update` condition that passes function value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: excess interest rate (rate-10.0) = 2.5\n"
     ]
    }
   ],
   "source": [
    "# Function that calculates rate overage (amount over 10.0)\n",
    "def excess_int_rate(rate):\n",
    "    return (rate-10.0)\n",
    "\n",
    "print('Test: excess interest rate (rate-10.0) = '+str(excess_int_rate(12.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTableUpdate.update(condition = col(\"int_rate\") > 10.0,\n",
    " set = { excess_int_rate_col: excess_int_rate(col(\"int_rate\")) } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use SQL command to dump records that met our previous condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL command: SELECT * FROM delta.`s3a://sagemaker-us-east-1-572539092864/delta_demo/delta_schema_update/` WHERE int_rate > 10.0\n"
     ]
    }
   ],
   "source": [
    "sql_cmd = f\"SELECT * FROM delta.`{s3a_delta_update_uri}` WHERE int_rate > 10.0\"\n",
    "print(f'SQL command: {sql_cmd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+---------+--------+----------+-----------+------------+------------------+\n",
      "|      id|loan_amnt|funded_amnt|     term|int_rate|addr_state|loan_status|funding_type|   excess_int_rate|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+------------+------------------+\n",
      "|68407277|   3600.0|     3600.0|36 months|   13.99|        PA| Fully Paid| FullyFunded|              3.99|\n",
      "|68355089|  24700.0|    24700.0|36 months|   11.99|        SD| Fully Paid| FullyFunded|1.9900000000000002|\n",
      "|68341763|  20000.0|    20000.0|60 months|   10.78|        IL| Fully Paid| FullyFunded|0.7799999999999994|\n",
      "|66310712|  35000.0|    35000.0|60 months|   14.85|        NJ|    Current| FullyFunded|              4.85|\n",
      "|68476807|  10400.0|    10400.0|60 months|   22.45|        PA| Fully Paid| FullyFunded|             12.45|\n",
      "+--------+---------+-----------+---------+--------+----------+-----------+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records = spark.sql(sql_cmd)\n",
    "records.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve table history following schema modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      7|2022-02-16 18:03:19|  null|    null|   UPDATE|{predicate -> (ca...|null|    null|     null|          6|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.2....|\n",
      "|      6|2022-02-16 18:03:03|  null|    null|   UPDATE|{predicate -> (lo...|null|    null|     null|          5|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.2....|\n",
      "|      5|2022-02-16 18:02:45|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          4|  Serializable|        false|{numFiles -> 1, n...|        null|Apache-Spark/3.2....|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's retrieve table history AFTER the schema modifications\n",
    "\n",
    "history_update_df = deltaTableUpdate.history()\n",
    "history_update_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
